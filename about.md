---
layout: page
title: "About"
header-img: "img/about-bg.jpg"
order: 5
comments:	true
---

Hey :v: 

I'm a [Facebook AI Resident](https://research.fb.com/programs/facebook-ai-residency-program/) interested in formalizing relatively new tasks that are empirically well solved using neural networks (e.g. self-supervised representation learning, transfer learning, meta-learning, multi-task learning ...) using tools from information theory, statistical learning, and Bayesian modelling. My goal is to develop a formalism that can be used directly to improve and develop current machine learning models and algorithms.

My interests stem from a sinuous learning path in different domains, and might not be settled yet. If you're bored, here's a romanticized perspective of my past:

* This journey started as I applied to a biomedical engineering bachelors at [EPFL, Switzerland](https://www.epfl.ch/){:.mdLink}. At that time, my hope was to be able to formalize *why* humans would take certain decisions (~ the "loss" that individuals and evolution are optimizing) and model *how* the brain achieved it (~ the "architecture" and "optimizer"). After some internships in neuroscience and computational genomics, I realized that the genome and the brain of humans were far too complicated and not understood well enough to hope formalizing anything in a near future.
* This brought me to believe that initial steps towards formalizing decision theory, learning, and intelligence should focus on much simpler artificial models in well controlled environments. As a result I started specializing in supervised machine learning and optimization as an exchange student at [UBC, Canada](https://www.ubc.ca/){:.mdLink}.
* As I started learning about new successes in deep learning, I realized that although supervised learning could be somewhat formalized using tools from statistical learning and optimization, many successful methods could not. This was especially true for word embeddings, which blew up my mind : how could training a model on one task help generalizing on an *other* task ? What was so special about language modelling for word embeddings to exhibit linear analogy properties ? To answer these questions, I decided to work one year to get a better understanding of word embeddings and generalization in NLP. I first spent 6 months as a Data Scientist Trainee in [Grab, Singapore](https://en.wikipedia.org/wiki/Grab_(company)){:.mdLink} developing and putting into production semi-supervised and multi-lingual word embeddings for different internal NLP tasks. I then worked as a Research Assistant at the [University of Amsterdam, Netherlands](https://www.illc.uva.nl/){:.mdLink} analysing extrapolation in NLP.
* Armed with this background, I thought I was ready to formalize how training on one (unsupervised) task could help on an other : using a Bayesian probabilistic model that quantified how each tasks were related. As a result I joined a group specialized in Bayesian machine learning at [Cambridge University, UK](https://www.cam.ac.uk/){:.mdLink} as a [MPhil in Machine Learning](https://www.mlmi.eng.cam.ac.uk/){:.mdLink}.
* Although very appealing and beautiful, probabilistic graphical models can be hard to scale and come up with. At Facebook AI, I'm trying to formalize the self-supervised representation learning and generalization, while requiring less assumptions on the data generative process. To do so, I am developing tools at the intersection of probabilsitic models and information theory ...

In my free-time I like to play many sports (basketball, volleyball, bouldering, rowing: you name it), watch machine learning MOOCs and re-implementing and improving cool papers (*e.g.* [Disentangling VAE](https://github.com/YannDubs/disentangling-vae){:.mdLink}, [Hash Embeddings](https://github.com/YannDubs/Hash-Embeddings){:.mdLink}). 

PS: Check out my [resume](/resume.pdf){:.mdLink} for more information.

PSx2: This picture was taken a loooooong time ago, but I thought it was appropriate for this page :telephone_receiver: :blush:.
